log/default/ADCprecision=5/batch_size=500/cellBit=4/dataset=cifar10/decreasing_lr=140,180/detect=0/grad_scale=8/inference=1/lr=0.01/mode=WAGE/model=ResNet20/onoffratio=10/seed=117/subArray=128/t=0/target=0/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=8/wl_weight=8
Building CIFAR-10 data loader with 1 workers
Files already downloaded and verified
Files already downloaded and verified
fan_in     27, float_limit 0.333333, float std 0.272166, quant limit 1.5, scale 4
fan_in    144, float_limit 0.144338, float std 0.117851, quant limit 1.5, scale 8
fan_in    144, float_limit 0.144338, float std 0.117851, quant limit 1.5, scale 8
fan_in    144, float_limit 0.144338, float std 0.117851, quant limit 1.5, scale 8
fan_in    144, float_limit 0.144338, float std 0.117851, quant limit 1.5, scale 8
fan_in    144, float_limit 0.144338, float std 0.117851, quant limit 1.5, scale 8
fan_in    144, float_limit 0.144338, float std 0.117851, quant limit 1.5, scale 8
fan_in    144, float_limit 0.144338, float std 0.117851, quant limit 1.5, scale 8
fan_in    288, float_limit 0.102062, float std 0.083333, quant limit 1.5, scale 16
fan_in    288, float_limit 0.102062, float std 0.083333, quant limit 1.5, scale 16
fan_in    288, float_limit 0.102062, float std 0.083333, quant limit 1.5, scale 16
fan_in    288, float_limit 0.102062, float std 0.083333, quant limit 1.5, scale 16
fan_in    288, float_limit 0.102062, float std 0.083333, quant limit 1.5, scale 16
fan_in    288, float_limit 0.102062, float std 0.083333, quant limit 1.5, scale 16
fan_in    576, float_limit 0.072169, float std 0.058926, quant limit 1.5, scale 16
fan_in    576, float_limit 0.072169, float std 0.058926, quant limit 1.5, scale 16
fan_in    576, float_limit 0.072169, float std 0.058926, quant limit 1.5, scale 16
fan_in    576, float_limit 0.072169, float std 0.058926, quant limit 1.5, scale 16
fan_in    576, float_limit 0.072169, float std 0.058926, quant limit 1.5, scale 16
fan_in     64, float_limit 0.216506, float std 0.176777, quant limit 1.5, scale 8
------------------------------ FloorPlan --------------------------------

Tile and PE size are optimized to maximize memory utilization ( = memory mapped by synapse / total memory on chip)

Desired Conventional Mapped Tile Storage Size: 16x16
Desired Conventional PE Storage Size: 8x8
Desired Novel Mapped Tile Storage Size: 9x8x8
User-defined SubArray Size: 2x2

----------------- # of tile used for each layer -----------------
layer1: 8
layer2: 16
layer3: 16
layer4: 16
layer5: 16
layer6: 16
layer7: 16
layer8: 32
layer9: 64
layer10: 64
layer11: 64
layer12: 64
layer13: 64
layer14: 128
layer15: 256
layer16: 256
layer17: 256
layer18: 256
layer19: 256
layer20: 12

----------------- Speed-up of each layer ------------------
layer1: 2
layer2: 1
layer3: 1
layer4: 1
layer5: 1
layer6: 1
layer7: 1
layer8: 1
layer9: 1
layer10: 1
layer11: 1
layer12: 1
layer13: 1
layer14: 1
layer15: 1
layer16: 1
layer17: 1
layer18: 1
layer19: 1
layer20: 1

----------------- Utilization of each layer ------------------
layer1: 0.75
layer2: 1
layer3: 1
layer4: 1
layer5: 1
layer6: 1
layer7: 1
layer8: 1
layer9: 1
layer10: 1
layer11: 1
layer12: 1
layer13: 1
layer14: 1
layer15: 1
layer16: 1
layer17: 1
layer18: 1
layer19: 1
layer20: 0.833333
Memory Utilization of Whole Chip: 99.7868 % 

---------------------------- FloorPlan Done ------------------------------



[NewSwitchMatrix] Error: pass gate height is even larger than the array height
ERROR: Height of subArray is even smaller than a single DFF !!! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
[Adder] Error: A single adder width is even larger than the assigned width ! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
[Adder] Error: A single adder width is even larger than the assigned width ! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
[NewSwitchMatrix] Error: pass gate height is even larger than the array height
ERROR: Height of subArray is even smaller than a single DFF !!! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
[Adder] Error: A single adder width is even larger than the assigned width ! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
[Adder] Error: A single adder width is even larger than the assigned width ! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
[Adder] Error: A single adder width is even larger than the assigned width ! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
[NewSwitchMatrix] Error: pass gate height is even larger than the array height
ERROR: Height of subArray is even smaller than a single DFF !!! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
[Adder] Error: A single adder width is even larger than the assigned width ! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
[Adder] Error: A single adder width is even larger than the assigned width ! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
[Adder] Error: A single adder width is even larger than the assigned width ! 
ERROR: Width of subArray is even smaller than a single DFF !!! 
-------------------------------------- Hardware Performance --------------------------------------
-------------------- Estimation of Layer 1 ----------------------
quantize layer  Conv_0_
input_shape = (500, 3, 32, 32)
quantize layer  Conv3x3_1_
input_shape = (500, 16, 32, 32)
quantize layer  Conv3x3_2_
input_shape = (500, 16, 32, 32)
quantize layer  Conv3x3_3_
input_shape = (500, 16, 32, 32)
quantize layer  Conv3x3_4_
input_shape = (500, 16, 32, 32)
quantize layer  Conv3x3_5_
input_shape = (500, 16, 32, 32)
quantize layer  Conv3x3_6_
input_shape = (500, 16, 32, 32)
quantize layer  Conv3x3_7_
input_shape = (500, 16, 32, 32)
quantize layer  Conv3x3_8_
input_shape = (500, 32, 16, 16)
quantize layer  Conv3x3_9_
input_shape = (500, 32, 16, 16)
quantize layer  Conv3x3_10_
input_shape = (500, 32, 16, 16)
quantize layer  Conv3x3_11_
input_shape = (500, 32, 16, 16)
quantize layer  Conv3x3_12_
input_shape = (500, 32, 16, 16)
quantize layer  Conv3x3_13_
input_shape = (500, 32, 16, 16)
quantize layer  Conv3x3_14_
input_shape = (500, 64, 8, 8)
quantize layer  Conv3x3_15_
input_shape = (500, 64, 8, 8)
quantize layer  Conv3x3_16_
input_shape = (500, 64, 8, 8)
quantize layer  Conv3x3_17_
input_shape = (500, 64, 8, 8)
quantize layer  Conv3x3_18_
input_shape = (500, 64, 8, 8)
quantize layer  FC_19_
 --- Hardware Properties --- 
subArray size: 
128
ADC precision: 
5
cell precision: 
4
on/off ratio: 
10
variation: 
0
Test set: 
avg loss: 
2.1815545558929443
accuracy: 
26.969999313354492
